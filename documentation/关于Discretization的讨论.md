# 关于Discretization的讨论

通过gym库，我们可以搭建起environment和agent，并且结合程序的循环，环境的变化和agent也能够得以实现。通过计算机和程序的性能，所有的运算过程和结果都可以近似看成是离散的，即使它们从根本上来说是离散的。

在本次项目中，使用RL算法，获得policy以及探究状态与状态之间是如何转换的就是特别重要的，因此我们必须要使用能够计算离散状态的RL算法，即Q-Learning算法。我们将建立一个有限的表格，有限不仅意味着表格的数量是有限的，还意味着我们能够通过现有的硬件性能，较为方便且快速的去训练它。我们将在这个章节讨论离散化的不同方式，对比它们的优缺点，从而能够更好的帮助下一步的训练过程。

## 两种方式

在讨论中，我们想到了两种方式，第一种是修改Environment的class，另一种是不修改class，而是去对environment进行sample。

第一种方式：将class打开，程序不仅可以读取class中的状态，并且可以对class种agent的state进行修改，让class直接与训练算法融合起来。

第二种方式：如之前所提及的gym的example实现方式，environment以及agent的控制和状态全部被封装到class内部。外部的程序只允许给这个agent固定的action，并且读取从该class返回的状态，而不允许直接修改agent的状态，以及如何render的方式。



## 讨论区别和优缺点

从直观理解角度方面讲，第二种方案更符合实际研究机器人算法的方式。在现实世界中，机器人的内部是复杂的且没有办法完全掌握的，因此将env/agent与训练控制流程分开做的方式，更加直观。第一种方案，虽然不符合实际机器人的训练方式，但却是在计算机角度来看，较为容易的方式。



从结构上来讲，第一种方案的结构更加线性化，由于agent的状态直接和训练算法连在一起。在一次循环中，就能一次性实现训练、控制机器人、训练算法、显示输出。它比较第二种方式而言，更加简洁和紧凑。

第二种方案则发挥了OOP语言的优势，将state封装到了类的内部，外部程序只能对class调用并且指定特定的action。由于内部相对于外部是一个连续的过程，因此在使用agent的state的时候，需要对state进行特定的转换，以满足训练算法的要求。虽然失去了紧凑的结构，但是整个程序将更加直观。



从易用性和拓展性来讲，第一种方案是“牵一发而动全身的”（pull one hair and it moves the whole body）。如果要修改机器人的运动方式，增加新的和环境的互动方式，或增加新的状态，那么就需要同时修改class和外部程序。它的复杂程度很高。第二种方式在易用性和拓展性上更占优，如果需要更新机器人的行为模式，只需要修改class的内部就能够达成目的。外部可以根据需要修改sample和训练的方法，就能很高效的获得最后的policy。



## 实践

经过讨论，两种模式都得到了实践。在class成功搭建的情况下，我们分别按照不同的方案对程序进行修改，以满足训练算法程序的要求。

第一种方案，将class中的step分开为step和step_render，前者是为了实际训练，后者则是为了能够将状态可视化。由于可视化的函数是直接读取state变量，而训练需要的state则是离散的，因此如果可视化离散的state，屏幕展示效果会是不连续的。因此step_render将直接控制class内的state，从而实现现实效果连续，训练方法离散。

第二种方案，保持class的完整，通过在训练算法外加if函数的方法，使训练算法可以对environment和agent进行sample。为了能够让训练算法获得获得特定状态，还需要对step函数的return值进行少量的修改，让连续的值的状态值相对离散化。



## 结论